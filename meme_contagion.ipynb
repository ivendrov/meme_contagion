{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "DATA_DIRECTORY = \"<FILL IN>\"\n",
    "\n",
    "def process_tweet_text(text):\n",
    "    # Remove t.co URLs\n",
    "    text = re.sub(r'https://t\\.co/\\w+', '', text)\n",
    "    # Remove any resulting double spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Trim whitespace and lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "def find_trending_tokens(recent_tokens, old_tokens, total_recent, total_old):\n",
    "    trending_scores = {}\n",
    "    \n",
    "    for token in recent_tokens:\n",
    "        recent_freq = recent_tokens[token] / total_recent\n",
    "        old_freq = (old_tokens[token] + 1) / total_old  # Add 1 for smoothing\n",
    "        expected = old_freq\n",
    "        trending_scores[token] = (recent_freq - expected) # ** 2 / expected\n",
    "        \n",
    "        # Calculate trending score\n",
    "        #trending_scores[token] = recent_freq / (old_freq ** 0.5)  # Using sqrt for log effect\n",
    "    \n",
    "    # Get top 100 trending tokens\n",
    "    top_trending = sorted(trending_scores.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "    return top_trending\n",
    "\n",
    "\n",
    "def load_tweet_data(json_files):\n",
    "    \"\"\"Load all tweet data into memory for reuse.\n",
    "    Returns list of (text, date, username) tuples for non-retweet, non-reply tweets.\"\"\"\n",
    "    tweet_data = []\n",
    "    \n",
    "    for json_file in tqdm.tqdm(json_files):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tweets = data.get('tweets', [])\n",
    "        print(\"Loaded \" + str(len(tweets)) + \"tweets\")\n",
    "        \n",
    "        for tweet_obj in tweets:\n",
    "            if 'tweet' not in tweet_obj or 'full_text' not in tweet_obj['tweet']:\n",
    "                continue\n",
    "                \n",
    "            text = tweet_obj['tweet']['full_text']\n",
    "            # Skip retweets and replies\n",
    "            if text.startswith('RT @') or text.startswith('@'):\n",
    "                continue\n",
    "                \n",
    "            # Get tweet timestamp\n",
    "            created_at = tweet_obj['tweet'].get('created_at')\n",
    "            if not created_at:\n",
    "                continue\n",
    "            \n",
    "            username = json_file.split('/')[-2]\n",
    "            if not username:\n",
    "                continue\n",
    "                \n",
    "            tweet_date = datetime.strptime(created_at, '%a %b %d %H:%M:%S %z %Y')\n",
    "            text = process_tweet_text(text)\n",
    "            \n",
    "            if text:  # Only include if there's text after filtering\n",
    "                tweet_data.append((text, tweet_date, username))\n",
    "    \n",
    "    return tweet_data\n",
    "def analyze_trending_tokens(tweet_data):\n",
    "    \"\"\"Analyze trending tokens from preprocessed tweet data.\"\"\"\n",
    "    recent_tokens = Counter()\n",
    "    old_tokens = Counter()\n",
    "    total_recent = 0\n",
    "    total_old = 0\n",
    "    \n",
    "    cutoff_date = datetime.now(datetime.now().astimezone().tzinfo) - timedelta(days=360)\n",
    "    \n",
    "    # Process tweets\n",
    "    for text, tweet_date, _ in tweet_data:\n",
    "        tokens = text.split()\n",
    "        if tweet_date > cutoff_date:\n",
    "            recent_tokens.update(tokens)\n",
    "            total_recent += len(tokens)\n",
    "        else:\n",
    "            old_tokens.update(tokens)\n",
    "            total_old += len(tokens)\n",
    "    \n",
    "    # Find tokens with 10x growth and at least 10 recent occurrences\n",
    "    trending_tokens = []\n",
    "    for token in recent_tokens:\n",
    "        recent_freq = recent_tokens[token] / total_recent\n",
    "        old_freq = (old_tokens[token] + 1) / total_old  # Add 1 for smoothing\n",
    "        \n",
    "        if recent_freq > (old_freq * 10) and recent_tokens[token] >= 10:\n",
    "            trending_tokens.append({\n",
    "                'token': token,\n",
    "                'recent_count': recent_tokens[token],\n",
    "                'old_count': old_tokens[token],\n",
    "                'recent_freq': recent_freq,\n",
    "                'old_freq': old_freq,\n",
    "                'growth': recent_freq / old_freq\n",
    "            })\n",
    "    \n",
    "    # Sort by recent frequency (popularity)\n",
    "    trending_tokens.sort(key=lambda x: x['recent_count'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTokens that grew >10x, sorted by recent popularity:\")\n",
    "    print(\"\\ntoken: recent_count (old_count) - growth_factor\")\n",
    "    for t in trending_tokens:\n",
    "        print(f\"{t['token']}: {t['recent_count']} ({t['old_count']}) - {t['growth']:.1f}x\")\n",
    "    \n",
    "    return trending_tokens, recent_tokens, old_tokens, total_recent, total_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(DATA_DIRECTORY + 'downloads/archives/*/*.json')\n",
    "if not json_files:\n",
    "    print(\"No JSON files found in ../data/downloads/archives/\")\n",
    "\n",
    "print(\"Loading tweet data...\")\n",
    "tweet_data = load_tweet_data(json_files)\n",
    "print(f\"Loaded {len(tweet_data)} tweets\")\n",
    "\n",
    "# Save tweet data to disk as a pickle file\n",
    "import pickle\n",
    "print(\"Saving tweet data to disk...\")\n",
    "with open(DATA_DIRECTORY + 'tweet_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tweet_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contagion modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_text(text):\n",
    "    # Remove t.co URLs\n",
    "    text = re.sub(r'https://t\\.co/\\w+', '', text)\n",
    "    # Remove any resulting double spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Trim whitespace and lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split text into tokens, handling punctuation and case.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on punctuation and whitespace\n",
    "    tokens = re.split(r'[\\s\\.,!?;:\"\\'()\\[\\]{}|/\\\\+=\\-_~`@#$%^&*]+', text)\n",
    "    # Remove empty tokens\n",
    "    return [t for t in tokens if t]\n",
    "\n",
    "def analyze_contagions(tweet_data):\n",
    "    token_first_use = {}  # token -> (date, username)\n",
    "    token_users = {}      # token -> set of usernames\n",
    "    token_uses = Counter()  # token -> total uses count\n",
    "    \n",
    "    sorted_tweets = sorted(tweet_data, key=lambda x: x[1])\n",
    "    \n",
    "    for text, date, username in tqdm.tqdm(sorted_tweets):\n",
    "        # Use new tokenize function\n",
    "        tokens = set(tokenize(text))  # use set to handle each token once per tweet\n",
    "        for token in tokens:\n",
    "            if not token:  # Skip empty tokens\n",
    "                continue\n",
    "                \n",
    "            if token not in token_first_use:\n",
    "                token_first_use[token] = (date, username)\n",
    "                token_users[token] = set()\n",
    "            \n",
    "            token_users[token].add(username)\n",
    "            token_uses[token] += 1\n",
    "    \n",
    "    # Find tokens that match our criteria\n",
    "    contagions = []\n",
    "    for token, (first_date, first_user) in token_first_use.items():\n",
    "        # Skip if first use before 2023 or not enough users\n",
    "        if first_date.year < 2018 or len(token_users[token]) < 30:\n",
    "            continue\n",
    "            \n",
    "        contagions.append({\n",
    "            'token': token,\n",
    "            'patient_zero': first_user,\n",
    "            'first_date': first_date,\n",
    "            'total_users': len(token_users[token]),\n",
    "            'total_uses': token_uses[token]\n",
    "        })\n",
    "    \n",
    "    # Sort by number of eventual users\n",
    "    contagions.sort(key=lambda x: x['total_users'], reverse=True)\n",
    "    \n",
    "    print(\"\\nViral tokens that spread from single origin in 2024:\")\n",
    "    print(\"\\ntoken: first_user (first_date) - total_users/total_uses\")\n",
    "    for c in contagions:\n",
    "        print(f\"{c['token']}: {c['patient_zero']} ({c['first_date'].strftime('%Y-%m-%d')}) - {c['total_users']}/{c['total_uses']} users/uses\")\n",
    "    \n",
    "    return contagions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_contagions_detailed(tweet_data):\n",
    "    token_first_use = {}  # token -> (date, username)\n",
    "    token_history = defaultdict(list)  # token -> list of (date, username, full_text)\n",
    "    user_adoption_ranks = defaultdict(list)  # username -> list of percentile ranks\n",
    "    \n",
    "    sorted_tweets = sorted(tweet_data, key=lambda x: x[1])\n",
    "    \n",
    "    # First pass: collect full history\n",
    "    for text, date, username in sorted_tweets:\n",
    "        tokens = set(tokenize(text))\n",
    "        for token in tokens:\n",
    "            if not token:\n",
    "                continue\n",
    "                \n",
    "            if token not in token_first_use:\n",
    "                token_first_use[token] = (date, username)\n",
    "            token_history[token].append((date, username, text))\n",
    "    \n",
    "    # Second pass: analyze viral tokens and compute adoption ranks\n",
    "    viral_tokens = []\n",
    "    for token, history in token_history.items():\n",
    "        # Sort history by date first\n",
    "        history.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Get total unique users for this token\n",
    "        total_users = len(set(username for _, username, _ in history))\n",
    "        \n",
    "        if total_users >= 30 and token_first_use[token][0].year >= 2018:\n",
    "            unique_users = []\n",
    "            seen_users = set()\n",
    "            current_rank = 0\n",
    "            \n",
    "            for _, username, _ in history:\n",
    "                if username not in seen_users:\n",
    "                    current_rank += 1\n",
    "                    unique_users.append(username)\n",
    "                    seen_users.add(username)\n",
    "                    \n",
    "                    # Calculate percentile rank based on final total users\n",
    "                    percentile = (current_rank - 1) / (total_users - 1)  # -1 to make range 0 to 1\n",
    "                    user_adoption_ranks[username].append(percentile)\n",
    "            \n",
    "            viral_tokens.append({\n",
    "                'token': token,\n",
    "                'first_use': token_first_use[token],\n",
    "                'second_user': unique_users[1] if len(unique_users) > 1 else None,\n",
    "                'total_users': total_users,\n",
    "                'unique_users': unique_users,\n",
    "                'full_history': history\n",
    "            })\n",
    "    \n",
    "    # Calculate user leaderboard stats\n",
    "    leaderboard = []\n",
    "    for username, ranks in user_adoption_ranks.items():\n",
    "        n = len(ranks)\n",
    "        mean = np.mean(ranks)\n",
    "        \n",
    "            \n",
    "        leaderboard.append({\n",
    "            'username': username,\n",
    "            'mean_percentile': mean,\n",
    "            'num_words': n,\n",
    "        })\n",
    "    \n",
    "    # Sort viral tokens by total users and leaderboard by mean percentile\n",
    "    viral_tokens.sort(key=lambda x: x['total_users'], reverse=True)\n",
    "    leaderboard.sort(key=lambda x: x['mean_percentile'])\n",
    "    \n",
    "    return viral_tokens, leaderboard\n",
    "\n",
    "def generate_html_report(viral_tokens, leaderboard):\n",
    "    html = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            .token-card { \n",
    "                border: 1px solid #ddd; \n",
    "                margin: 5px 0; \n",
    "                padding: 10px; \n",
    "            }\n",
    "            .token-header { \n",
    "                display: flex; \n",
    "                justify-content: space-between; \n",
    "                align-items: center;\n",
    "                gap: 20px;\n",
    "            }\n",
    "            .token-header > * {\n",
    "                margin: 0;\n",
    "                white-space: nowrap;\n",
    "            }\n",
    "            .details { display: none; margin-top: 10px; }\n",
    "            .show-details { cursor: pointer; color: blue; }\n",
    "            .leaderboard { margin-top: 30px; }\n",
    "            table { border-collapse: collapse; width: 100%; }\n",
    "            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "        </style>\n",
    "        <script>\n",
    "            function toggleDetails(id) {\n",
    "                var details = document.getElementById(id);\n",
    "                details.style.display = details.style.display === 'none' ? 'block' : 'none';\n",
    "            }\n",
    "        </script>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Viral Token Analysis</h1>\n",
    "        \n",
    "        <h2>Token Timeline</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add token cards\n",
    "    for i, token in enumerate(viral_tokens):\n",
    "        html += f\"\"\"\n",
    "        <div class=\"token-card\">\n",
    "            <div class=\"token-header\">\n",
    "                <strong style=\"min-width: 150px;\">{token['token']}</strong>\n",
    "                <span>First: {token['first_use'][1]} ({token['first_use'][0].strftime('%Y-%m-%d')})</span>\n",
    "                <span>Second: {token['second_user'] or 'N/A'}</span>\n",
    "                <span>Total Users: {token['total_users']}</span>\n",
    "                <span class=\"show-details\" onclick=\"toggleDetails('details-{i}')\">Show Details</span>\n",
    "            </div>\n",
    "            <div id=\"details-{i}\" class=\"details\">\n",
    "                <h4>All Users (in order):</h4>\n",
    "                <ol>\n",
    "        \"\"\"\n",
    "        seen_users = set()\n",
    "        for _, username, _ in token['full_history']:\n",
    "            if username not in seen_users:\n",
    "                html += f\"<li>{username}</li>\"\n",
    "                seen_users.add(username)\n",
    "        \n",
    "        html += \"\"\"\n",
    "                </ol>\n",
    "                <h4>All Tweets (chronological):</h4>\n",
    "                <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        for date, username, text in token['full_history']:\n",
    "            html += f\"<li><b>{username}</b> ({date.strftime('%Y-%m-%d')}): {text}</li>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add leaderboard\n",
    "    html += \"\"\"\n",
    "        <h2>Early Adopter Leaderboard</h2>\n",
    "        <table class=\"leaderboard\">\n",
    "            <tr>\n",
    "                <th>Username</th>\n",
    "                <th>Average Percentile</th>\n",
    "                <th>Number of Words</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for user in leaderboard:\n",
    "        html += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{user['username']}</td>\n",
    "                <td>{user['mean_percentile']:.3f}</td>\n",
    "                <td>{user['num_words']}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "# Generate and save the report\n",
    "#viral_tokens, leaderboard = analyze_contagions_detailed(tweet_data)\n",
    "html_report = generate_html_report(viral_tokens, leaderboard)\n",
    "\n",
    "with open('viral_tokens_report2.html', 'w') as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "print(\"Report generated as viral_tokens_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Create dictionaries for monthly stats\n",
    "monthly_users = defaultdict(set)  # month -> set of unique users\n",
    "monthly_tweets = defaultdict(int) # month -> total tweet count\n",
    "\n",
    "for _, tweet_date, username in tweet_data:\n",
    "    # Create a year-month key (e.g., \"2024-01\")\n",
    "    month_key = tweet_date.strftime(\"%Y-%m\")\n",
    "    monthly_users[month_key].add(username)\n",
    "    monthly_tweets[month_key] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'unique_users': {k: len(v) for k, v in monthly_users.items()},\n",
    "    'total_tweets': monthly_tweets\n",
    "})\n",
    "df.index.name = 'month'\n",
    "df = df.sort_index()\n",
    "\n",
    "# Add tweets per user column\n",
    "df['tweets_per_user'] = df['total_tweets'] / df['unique_users']\n",
    "\n",
    "# Display results\n",
    "print(\"\\nMonthly statistics:\")\n",
    "print(df)\n",
    "\n",
    "# Find the month with most unique users\n",
    "max_month = df['unique_users'].idxmax()\n",
    "print(f\"\\nMonth with most unique users: {max_month}\")\n",
    "print(f\"Unique users: {df.loc[max_month, 'unique_users']:,}\")\n",
    "print(f\"Total tweets: {df.loc[max_month, 'total_tweets']:,}\")\n",
    "print(f\"Tweets per user: {df.loc[max_month, 'tweets_per_user']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get all tweets from August 2024\n",
    "august_tweets = []\n",
    "for text, date, username in tweet_data:\n",
    "    if date.strftime(\"%Y-%m\") == \"2024-08\":\n",
    "        august_tweets.append(text)\n",
    "\n",
    "print(f\"Found {len(august_tweets)} tweets from August 2024\")\n",
    "\n",
    "# Load the model on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model.to(device)\n",
    "\n",
    "# Embed tweets in batches with progress bar\n",
    "batch_size = 256  # Increased batch size since we're using GPU\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(august_tweets), batch_size)):\n",
    "    batch = august_tweets[i:i + batch_size]\n",
    "    batch_embeddings = model.encode(batch, device=device)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "# Combine all batches\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "# Save embeddings to disk\n",
    "np.save('august_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "# First create a mapping of original indices to embedding indices for August tweets\n",
    "august_index_map = {}  # Maps original index to embedding index\n",
    "august_user_tweets = defaultdict(list)  # Maps username to list of embedding indices\n",
    "\n",
    "embedding_idx = 0\n",
    "for orig_idx, (text, date, username) in enumerate(tweet_data):\n",
    "    if date.strftime(\"%Y-%m\") == \"2024-08\":\n",
    "        august_index_map[orig_idx] = embedding_idx\n",
    "        august_user_tweets[username].append(embedding_idx)\n",
    "        embedding_idx += 1\n",
    "\n",
    "# Get list of users with enough tweets\n",
    "min_tweets = 5\n",
    "users = [user for user, indices in august_user_tweets.items() if len(indices) >= min_tweets]\n",
    "\n",
    "# Convert embeddings to torch tensor on GPU\n",
    "embeddings_tensor = torch.tensor(embeddings, device='cuda')\n",
    "# Normalize embeddings for faster cosine similarity\n",
    "embeddings_tensor = embeddings_tensor / embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Create similarity matrix between users\n",
    "n_users = len(users)\n",
    "user_sim_matrix = np.zeros((n_users, n_users))\n",
    "\n",
    "# Calculate similarities using batch matrix multiplication\n",
    "for i, user1 in tqdm.tqdm(enumerate(users)):\n",
    "    for j, user2 in enumerate(users):\n",
    "        if i == j:\n",
    "            # For same user, calculate average similarity between different tweets\n",
    "            tweets1 = embeddings_tensor[august_user_tweets[user1]]\n",
    "            # Compute all pairwise similarities at once\n",
    "            sim_matrix = torch.mm(tweets1, tweets1.T)\n",
    "            # Mask out self-similarities\n",
    "            mask = torch.triu(torch.ones_like(sim_matrix), diagonal=1)\n",
    "            similarities = sim_matrix[mask.bool()]\n",
    "            user_sim_matrix[i, j] = similarities.mean().cpu().item() if len(similarities) > 0 else 0\n",
    "        else:\n",
    "            # For different users, calculate average similarity between all tweet pairs\n",
    "            tweets1 = embeddings_tensor[august_user_tweets[user1]]\n",
    "            tweets2 = embeddings_tensor[august_user_tweets[user2]]\n",
    "            # Compute all similarities at once\n",
    "            similarities = torch.mm(tweets1, tweets2.T)\n",
    "            user_sim_matrix[i, j] = similarities.mean().cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(22,20))\n",
    "sns.heatmap(user_sim_matrix, \n",
    "            xticklabels=users,\n",
    "            yticklabels=users,\n",
    "            cmap='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Average Tweet Similarity Between Users')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some stats\n",
    "diagonal_mean = np.mean(np.diag(user_sim_matrix))\n",
    "offdiagonal_mean = np.mean(user_sim_matrix[~np.eye(user_sim_matrix.shape[0], dtype=bool)])\n",
    "print(f\"Average within-user similarity: {diagonal_mean:.3f}\")\n",
    "print(f\"Average between-user similarity: {offdiagonal_mean:.3f}\")\n",
    "print(f\"Identity strength ratio: {diagonal_mean/offdiagonal_mean:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user, count how many other users have higher average similarity\n",
    "more_similar_counts = []\n",
    "for i, user in enumerate(users):\n",
    "    self_similarity = user_sim_matrix[i, i]\n",
    "    # Count users with higher similarity (excluding self)\n",
    "    more_similar = np.sum(user_sim_matrix[:, i] > self_similarity) - 1  # -1 to exclude self\n",
    "    more_similar_counts.append(more_similar)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(more_similar_counts, bins=30, edgecolor='black')\n",
    "plt.title('Distribution of Users with Higher Cross-Similarity than Self-Similarity')\n",
    "plt.xlabel('Number of Users More Similar')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(np.mean(more_similar_counts), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(more_similar_counts):.1f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average number of users more similar than self: {np.mean(more_similar_counts):.1f}\")\n",
    "print(f\"Median number of users more similar than self: {np.median(more_similar_counts):.1f}\")\n",
    "print(f\"Percentage of users with at least one more similar user: {np.mean(np.array(more_similar_counts) > 0) * 100:.1f}%\")\n",
    "# Create leaderboard data\n",
    "leaderboard = []\n",
    "for i, user in enumerate(users):\n",
    "    self_similarity = user_sim_matrix[i, i]\n",
    "    more_similar = np.sum(user_sim_matrix[:, i] > self_similarity) - 1  # -1 to exclude self\n",
    "    n_tweets = len(august_user_tweets[user])\n",
    "    leaderboard.append({\n",
    "        'username': user,\n",
    "        'more_similar_users': more_similar,\n",
    "        'self_similarity': self_similarity,\n",
    "        'tweet_count': n_tweets\n",
    "    })\n",
    "\n",
    "# Sort by number of more similar users\n",
    "leaderboard.sort(key=lambda x: x['more_similar_users'], reverse=True)\n",
    "\n",
    "# Print top and bottom of leaderboard\n",
    "print(\"Users with LEAST distinct tweet identity (most users more similar than self):\")\n",
    "print(\"\\nRank  Username                More Similar Users  Self-Similarity  Tweet Count\")\n",
    "print(\"-\" * 75)\n",
    "for i, entry in enumerate(leaderboard[:20]):\n",
    "    print(f\"{i+1:4d}  {entry['username']:<22} {entry['more_similar_users']:17d} {entry['self_similarity']:14.3f} {entry['tweet_count']:11d}\")\n",
    "\n",
    "print(\"\\n\\nUsers with MOST distinct tweet identity (fewest users more similar than self):\")\n",
    "print(\"\\nRank  Username                More Similar Users  Self-Similarity  Tweet Count\")\n",
    "print(\"-\" * 75)\n",
    "# Sort by self-similarity.\n",
    "for i, entry in enumerate(sorted(leaderboard, key=lambda x: x['self_similarity'], reverse=True)[:20]):\n",
    "    print(f\"{len(leaderboard)-19+i:4d}  {entry['username']:<22} {entry['more_similar_users']:17d} {entry['self_similarity']:14.3f} {entry['tweet_count']:11d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# First get a week of data (let's take the most active week)\n",
    "weekly_counts = defaultdict(int)\n",
    "for text, date, username in tweet_data:\n",
    "    if date.strftime(\"%Y-%m\") == \"2024-08\":\n",
    "        week = date.strftime(\"%Y-%W\")  # ISO week number\n",
    "        weekly_counts[week] += 1\n",
    "\n",
    "# Get the busiest week\n",
    "busiest_week = max(weekly_counts.items(), key=lambda x: x[1])[0]\n",
    "print(f\"Using week {busiest_week} with {weekly_counts[busiest_week]} tweets\")\n",
    "\n",
    "# Get indices for that week\n",
    "week_index_map = {}  # Maps original index to embedding index\n",
    "week_user_tweets = defaultdict(list)  # Maps username to list of embedding indices\n",
    "\n",
    "embedding_idx = 0\n",
    "for orig_idx, (text, date, username) in enumerate(tweet_data):\n",
    "    if date.strftime(\"%Y-%W\") == busiest_week:\n",
    "        week_index_map[orig_idx] = embedding_idx\n",
    "        week_user_tweets[username].append(embedding_idx)\n",
    "        embedding_idx += 1\n",
    "\n",
    "# Get embeddings for just that week\n",
    "week_embeddings = embeddings_tensor[list(week_index_map.values())]\n",
    "\n",
    "# Compute full similarity matrix on GPU\n",
    "print(\"Computing similarity matrix...\")\n",
    "sim_matrix = torch.mm(week_embeddings, week_embeddings.T)\n",
    "\n",
    "# Convert to distances (1 - similarity since vectors are normalized)\n",
    "dist_matrix = 1 - sim_matrix.cpu().numpy()\n",
    "\n",
    "# Ensure diagonal is exactly zero\n",
    "np.fill_diagonal(dist_matrix, 0)\n",
    "\n",
    "# Convert to condensed form (upper triangular, no diagonal)\n",
    "condensed_dist = squareform(dist_matrix)\n",
    "\n",
    "# Compute linkage using the condensed distance matrix\n",
    "print(\"Computing hierarchical clustering...\")\n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "\n",
    "# Create cluster points with jitter\n",
    "cluster_points = []\n",
    "n_samples = len(week_embeddings)\n",
    "\n",
    "print(f\"Starting with {n_samples} samples\")\n",
    "\n",
    "# Initialize: each point starts as its own cluster with one tweet\n",
    "cluster_tweets = {}  # Maps cluster index -> set of tweets\n",
    "for i in range(n_samples):\n",
    "    orig_idx = list(week_index_map.values()).index(i)\n",
    "    orig_idx = list(week_index_map.keys())[orig_idx]\n",
    "    cluster_tweets[i] = {(tweet_data[orig_idx][0], tweet_data[orig_idx][2])}  # (text, username)\n",
    "\n",
    "for i, merge in enumerate(linkage_matrix):\n",
    "    # Get indices of clusters/points being merged\n",
    "    left_idx = int(merge[0])\n",
    "    right_idx = int(merge[1])\n",
    "    new_idx = n_samples + i\n",
    "    \n",
    "    # Get the correct tweets based on whether it's an original point or merged cluster\n",
    "    if left_idx < n_samples:\n",
    "        left_tweets = cluster_tweets[left_idx]\n",
    "    else:\n",
    "        left_tweets = cluster_tweets[n_samples + int(left_idx - n_samples)]\n",
    "        \n",
    "    if right_idx < n_samples:\n",
    "        right_tweets = cluster_tweets[right_idx]\n",
    "    else:\n",
    "        right_tweets = cluster_tweets[n_samples + int(right_idx - n_samples)]\n",
    "    \n",
    "    # Store merged tweets\n",
    "    cluster_tweets[new_idx] = left_tweets.union(right_tweets)\n",
    "    \n",
    "    # Only create visualization points for clusters >= 3\n",
    "    if merge[3] >= 3:\n",
    "        cluster_points.append({\n",
    "            'size': int(merge[3]),\n",
    "            'similarity': float(1 - merge[2]),\n",
    "            'hover_text': f\"Natural Cluster<br>Size: {int(merge[3])}<br>Similarity: {1-merge[2]:.3f}\",\n",
    "            'tweets': [f\"@{username}: {text}\" for text, username in cluster_tweets[new_idx]]\n",
    "        })\n",
    "\n",
    "# Add jitter to similarities\n",
    "jitter = np.random.normal(0, 0.0001, len(cluster_points))\n",
    "for i, p in enumerate(cluster_points):\n",
    "    p['similarity'] += jitter[i]\n",
    " \n",
    "\n",
    "print(f\"Found {len(cluster_points)} natural clusters of size >= 3\")\n",
    "\n",
    "# Create user points with jitter\n",
    "user_points = []\n",
    "users = [u for u, tweets in week_user_tweets.items() if len(tweets) >= 5]\n",
    "jitter = np.random.normal(0, 0.0001, len(users))\n",
    "\n",
    "for i, username in enumerate(users):\n",
    "    indices = week_user_tweets[username]\n",
    "    n_tweets = len(indices)\n",
    "    \n",
    "    # Get similarities for this user's tweets\n",
    "    user_sim = sim_matrix[indices][:, indices]\n",
    "    mask = torch.triu(torch.ones_like(user_sim), diagonal=1)\n",
    "    similarities = user_sim[mask.bool()]\n",
    "    avg_similarity = similarities.mean().item() + jitter[i]\n",
    "    \n",
    "    # Get the actual tweets for this user\n",
    "    user_tweets = []\n",
    "    for idx in indices:\n",
    "        orig_idx = list(week_index_map.keys())[list(week_index_map.values()).index(idx)]\n",
    "        user_tweets.append(f\"{tweet_data[orig_idx][0]}\")\n",
    "    \n",
    "    user_points.append({\n",
    "        'size': n_tweets,\n",
    "        'similarity': avg_similarity,\n",
    "        'username': username,\n",
    "        'hover_text': f\"User: {username}<br>Tweets: {n_tweets}<br>Similarity: {avg_similarity:.3f}\",\n",
    "        'tweets': user_tweets\n",
    "    })\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add natural clusters\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[p['similarity'] for p in cluster_points],\n",
    "    y=[p['size'] for p in cluster_points],\n",
    "    mode='markers',\n",
    "    name='Natural Clusters',\n",
    "    marker=dict(color='gray', size=8, opacity=0.6),\n",
    "    hovertext=[p['hover_text'] for p in cluster_points],\n",
    "    hoverinfo='text',\n",
    "    customdata=list(range(len(cluster_points)))\n",
    "))\n",
    "\n",
    "# Add users\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[p['similarity'] for p in user_points],\n",
    "    y=[p['size'] for p in user_points],\n",
    "    mode='markers+text',\n",
    "    name='Users',\n",
    "    marker=dict(color='red', size=10, opacity=0.7),\n",
    "    hovertext=[p['hover_text'] for p in user_points],\n",
    "    hoverinfo='text',\n",
    "    text=[p['username'] if (p['size'] > np.percentile([u['size'] for u in user_points], 95) or \n",
    "                           p['similarity'] > np.percentile([u['similarity'] for u in user_points], 95))\n",
    "          else '' for p in user_points],\n",
    "    textposition=\"top right\",\n",
    "    customdata=list(range(len(cluster_points), len(cluster_points) + len(user_points)))\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'Natural Clusters vs User Clusters (Week {busiest_week})',\n",
    "    xaxis_title='Average Similarity',\n",
    "    yaxis_title='Cluster Size (# tweets)',\n",
    "    yaxis_type=\"log\",\n",
    "    hovermode='closest',\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    showlegend=True,\n",
    "    template='plotly_white',\n",
    "    margin=dict(r=300)\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "# Create the HTML with the same JavaScript as before\n",
    "# [Previous Python code stays exactly the same until the HTML template]\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "        .container { display: flex; }\n",
    "        .plot { flex: 7; }\n",
    "        .tweets { \n",
    "            flex: 3; \n",
    "            padding: 10px; \n",
    "            max-height: 800px; \n",
    "            overflow-y: auto; \n",
    "            border-left: 1px solid #ccc;\n",
    "        }\n",
    "        .tweet {\n",
    "            padding: 10px;\n",
    "            border-bottom: 1px solid #eee;\n",
    "            font-size: 14px;\n",
    "        }\n",
    "        .username {\n",
    "            font-weight: bold;\n",
    "            color: #1DA1F2;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"plot\" id=\"plot\"></div>\n",
    "        <div class=\"tweets\" id=\"tweetDisplay\"></div>\n",
    "    </div>\n",
    "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "    <script>\n",
    "        const userPoints = \"\"\" + json.dumps(user_points) + \"\"\";\n",
    "        const clusterPoints = \"\"\" + json.dumps(cluster_points) + \"\"\";\n",
    "        \n",
    "        const plot = \"\"\" + fig.to_json() + \"\"\";\n",
    "        Plotly.newPlot('plot', plot.data, plot.layout);\n",
    "\n",
    "        document.getElementById('plot').on('plotly_click', function(data) {\n",
    "            const point = data.points[0];\n",
    "            const idx = point.customdata;\n",
    "            let tweets;\n",
    "            let header;\n",
    "            \n",
    "            if (idx < clusterPoints.length) {\n",
    "                tweets = clusterPoints[idx].tweets;\n",
    "                header = `Natural Cluster (${clusterPoints[idx].size} tweets)`;\n",
    "            } else {\n",
    "                const userIdx = idx - clusterPoints.length;\n",
    "                tweets = userPoints[userIdx].tweets;\n",
    "                header = `User: ${userPoints[userIdx].username} (${userPoints[userIdx].size} tweets)`;\n",
    "            }\n",
    "            \n",
    "            let tweetHtml = `<h3>${header}</h3>`;\n",
    "            for (let i = 0; i < tweets.length; i++) {\n",
    "                tweetHtml += `\n",
    "                    <div class=\"tweet\">\n",
    "                        <div class=\"text\">${tweets[i]}</div>\n",
    "                    </div>\n",
    "                `;\n",
    "            }\n",
    "            \n",
    "            document.getElementById('tweetDisplay').innerHTML = tweetHtml;\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save to HTML\n",
    "with open(\"cluster_comparison.html\", \"w\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Number of natural clusters: {len(cluster_points)}\")\n",
    "print(f\"Number of users: {len(user_points)}\")\n",
    "print(f\"Average similarity: {np.mean([p['similarity'] for p in user_points]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cluster_tweets.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
